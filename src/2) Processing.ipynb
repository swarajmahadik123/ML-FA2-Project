{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ac918a-4577-4d03-bf98-385ca4dceac0",
   "metadata": {},
   "source": [
    "# **2.1 EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b51a2b-19ce-40c4-a967-07dd5e312ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "sns.set(style='darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the dataset\n",
    "data1 = pd.read_csv(r'E:/Random Python Scripts/CICIDS/CICIDS2017/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "data2 = pd.read_csv(r'E:/Random Python Scripts/CICIDS/CICIDS2017/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv')\n",
    "data3 = pd.read_csv(r'E:/Random Python Scripts/CICIDS/CICIDS2017/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv')\n",
    "data4 = pd.read_csv(r'E:/Random Python Scripts/CICIDS/CICIDS2017/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
    "data5 = pd.read_csv(r'E:/Random Python Scripts/CICIDS/CICIDS2017/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
    "data6 = pd.read_csv(r'E:/Random Python Scripts/CICIDS/CICIDS2017/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "data7 = pd.read_csv(r'E:/Random Python Scripts/CICIDS/CICIDS2017/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
    "data8 = pd.read_csv(r'E:/Random Python Scripts/CICIDS/CICIDS2017/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
    "\n",
    "data_list = [data1, data2, data3, data4, data5, data6, data7, data8]\n",
    "\n",
    "print('Data dimensions: ')\n",
    "for i, data in enumerate(data_list, start = 1):\n",
    "  rows, cols = data.shape\n",
    "  print(f'Data{i} -> {rows} rows, {cols} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6a12b-b07d-44e0-8cce-5071c4419837",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(data_list)\n",
    "rows, cols = data.shape\n",
    "\n",
    "print('New dimension:')\n",
    "print(f'Number of rows: {rows}')\n",
    "print(f'Number of columns: {cols}')\n",
    "print(f'Total cells: {rows * cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff54ea3-b093-4bb9-ad63-37f154449212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns by removing leading/trailing whitespace\n",
    "col_names = {col: col.strip() for col in data.columns}\n",
    "data.rename(columns = col_names, inplace = True)\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359de96-7f28-4823-8630-8b1c8547ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5295256-59a0-4814-8f0a-28936fc7246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 80\n",
    "\n",
    "print('Overview of Columns:')\n",
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e3fcf-f2fa-4976-8727-601e8794f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 80\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7da899-d39a-4bfd-953c-df39ef547df1",
   "metadata": {},
   "source": [
    "# **2.2 Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd707b-c171-4887-b8c2-4031d2436651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying duplicate values\n",
    "dups = data[data.duplicated()]\n",
    "print(f'Number of duplicates: {len(dups)}')\n",
    "\n",
    "data.drop_duplicates(inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f18d15-c693-4a32-93a2-ff85107a32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Missing Values\n",
    "missing_val = data.isna().sum()\n",
    "\n",
    "# Checking for infinity values\n",
    "numeric_cols = data.select_dtypes(include = np.number).columns\n",
    "inf_count = np.isinf(data[numeric_cols]).sum()\n",
    "\n",
    "# Replacing any infinite values (positive or negative) with NaN (not a number)\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "missing = data.isna().sum()\n",
    "\n",
    "# Calculating missing value percentage in the dataset\n",
    "mis_per = (missing / len(data)) * 100\n",
    "mis_table = pd.concat([missing, mis_per.round(2)], axis = 1)\n",
    "mis_table = mis_table.rename(columns = {0 : 'Missing Values', 1 : 'Percentage of Total Values'})\n",
    "\n",
    "med_flow_bytes = data['Flow Bytes/s'].median()\n",
    "med_flow_packets = data['Flow Packets/s'].median()\n",
    "\n",
    "# Filling missing values with median\n",
    "data['Flow Bytes/s'].fillna(med_flow_bytes, inplace = True)\n",
    "data['Flow Packets/s'].fillna(med_flow_packets, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01827bd3-0eba-4a66-98c5-4d2d6ba6e93d",
   "metadata": {},
   "source": [
    "The first step is to identify duplicate rows and missing or invalid values. We identified and dropped the duplicate rows (308381 rows). From the data description, we identified that the dataset has infinity values. So, we checked and replaced the positive or negative infinity values with NaN (not a number) and counted it as a missing value. In the dataset, two features, FlowBytes/s, and Flow Packets/s contain missing values.\n",
    "\n",
    "Flow Bytes/s and Flow Packets/s are continuous variables. The data is not normally distributed. The variables have extreme values or outliers. So, our strategy is to fill in missing values with median value. Because, filling the missing values with the median does not introduce any new categories or disrupt the distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854094e-0401-406c-a1c8-39e2d917b1ce",
   "metadata": {},
   "source": [
    "# **2.3 Visualization of column correlation. Also, plotting Heat Map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe113d1-2800-4785-aad3-0e354cdfc30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616f5a3-c109-4cc5-9893-259904e4865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of attacks & normal instances (BENIGN)\n",
    "data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd80fd-b87e-416b-96d9-6cd67a0dd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary that maps each label to its attack type\n",
    "attack_map = {\n",
    "    'BENIGN': 'BENIGN',\n",
    "    'DDoS': 'DDoS',\n",
    "    'DoS Hulk': 'DoS',\n",
    "    'DoS GoldenEye': 'DoS',\n",
    "    'DoS slowloris': 'DoS',\n",
    "    'DoS Slowhttptest': 'DoS',\n",
    "    'PortScan': 'Port Scan',\n",
    "    'FTP-Patator': 'Brute Force',\n",
    "    'SSH-Patator': 'Brute Force',\n",
    "    'Bot': 'Bot',\n",
    "    'Web Attack � Brute Force': 'Web Attack',\n",
    "    'Web Attack � XSS': 'Web Attack',\n",
    "    'Web Attack � Sql Injection': 'Web Attack',\n",
    "    'Infiltration': 'Infiltration',\n",
    "    'Heartbleed': 'Heartbleed'\n",
    "}\n",
    "\n",
    "# Creating a new column 'Attack Type' in the DataFrame based on the attack_map dictionary\n",
    "data['Attack Type'] = data['Label'].map(attack_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2d2ec-469d-45e9-84c5-16452a7268af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Attack Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574bed1-2dbb-4ce9-bfd0-f787acbd8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Label', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821d3be-16b3-4434-8c34-c49da5b88065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Attack Number'] = le.fit_transform(data['Attack Type'])\n",
    "\n",
    "print(data['Attack Number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a41b7e-5dd5-4b3d-9071-255df7c46441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing corresponding attack type for each encoded value\n",
    "encoded_values = data['Attack Number'].unique()\n",
    "for val in sorted(encoded_values):\n",
    "    print(f\"{val}: {le.inverse_transform([val])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4febd54-d653-4a87-a772-027fff8a92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr(numeric_only = True).round(2)\n",
    "corr.style.background_gradient(cmap = 'coolwarm', axis = None).format(precision = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1e26d-12f0-4e29-a4e4-7df21f668984",
   "metadata": {},
   "source": [
    "For plotting the correlation matrix, we encoded the 'Attack Type' column and plotted the heatmap. From the heatmap, we observe that there are many pairs of highly correlated features. Highly correlated features in the dataset are problematic and lead to overfitting. A positive correlation exists when one variable decreases as the other variable decreases or one variable increases while the other increases. There are 32 features with positive correlations that may help in predicting the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e3c41-a629-4304-9a9c-f68d9bc11c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (24, 24))\n",
    "sns.heatmap(corr, cmap = 'coolwarm', annot = False, linewidth = 0.5)\n",
    "plt.title('Correlation Matrix', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a409f-cdaa-4d27-9f83-26655502babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for columns with zero standard deviation (the blank squares in the heatmap)\n",
    "std = data.std(numeric_only = True)\n",
    "zero_std_cols = std[std == 0].index.tolist()\n",
    "zero_std_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ae896-8934-4138-89a6-4410d5c6fcf1",
   "metadata": {},
   "source": [
    "The columns with zero standard deviation have the same value in all rows. These columns don't have any variance. It simply means that there is no meaningful relationship with any other columns which results in NaN correlation cofficient. These columns cannot help differentiate between the classes or groups of data. So, these zero standard deviation columns don't contribute to the correlation matrix and will appear blank in the heatmap. This can be helpful while doing data processing as we may drop the columns if we find out that these columns has no variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3919e428-bd89-47d2-85b6-f66260acf928",
   "metadata": {},
   "source": [
    "# **3.1 Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879dce2e-0857-4756-b117-3db0a7a9f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# For improving performance and reduce memory-related errors\n",
    "old_memory_usage = data.memory_usage().sum() / 1024 ** 2\n",
    "print(f'Initial memory usage: {old_memory_usage:.2f} MB')\n",
    "for col in data.columns:\n",
    "    col_type = data[col].dtype\n",
    "    if col_type != object:\n",
    "        c_min = data[col].min()\n",
    "        c_max = data[col].max()\n",
    "        # Downcasting float64 to float32\n",
    "        if str(col_type).find('float') >= 0 and c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "            data[col] = data[col].astype(np.float32)\n",
    "\n",
    "        # Downcasting int64 to int32\n",
    "        elif str(col_type).find('int') >= 0 and c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "            data[col] = data[col].astype(np.int32)\n",
    "\n",
    "new_memory_usage = data.memory_usage().sum() / 1024 ** 2\n",
    "print(f\"Final memory usage: {new_memory_usage:.2f} MB\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e32698-4ea9-4748-9150-817b1f70c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with only one unique value\n",
    "num_unique = data.nunique()\n",
    "one_variable = num_unique[num_unique == 1]\n",
    "not_one_variable = num_unique[num_unique > 1].index\n",
    "\n",
    "dropped_cols = one_variable.index\n",
    "data = data[not_one_variable]\n",
    "\n",
    "print('Dropped columns:')\n",
    "dropped_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7ebc8-d214-4d00-b2c8-bca7425b09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c70c6-5b0f-4a94-9ed6-aec220a1b5f0",
   "metadata": {},
   "source": [
    "# **3.2 Applying PCA to Reduce Dimensions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac832da-6793-460a-a8a0-366de3030602",
   "metadata": {},
   "source": [
    "A simple and effective way to reduce the dimensionality of the dataset and improve the performance of the model is to use strongly correlated features. We used label encoding on the target feature where the numerical values assigned to each category do not have inherent meaning and they are arbitrary. For this reason, the correlation matrix calculated using label-encoded variables may not accurately reflect the true relationships between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69083830-8fc9-4c56-9a39-8aa4dfd97128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We applied StandardScaler before performing Incremental PCA to standardize the data values into a standard format.\n",
    "\n",
    "# Standardizing the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = data.drop('Attack Type', axis = 1)\n",
    "attacks = data['Attack Type']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f45573-97e9-4858-bed8-455ee49335d8",
   "metadata": {},
   "source": [
    "Incremental PCA is a variant of PCA that allows for the efficient computation of principal components of a large dataset that cannot be stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198de4c0-b3c7-4909-967c-d57b20f3bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "size = len(features.columns) // 2\n",
    "ipca = IncrementalPCA(n_components = size, batch_size = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435ace2-ae5b-4638-b64e-922eb4a2600d",
   "metadata": {},
   "source": [
    "Interrupt kernel for below code snippet after 150 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd35aea-e064-473c-9d90-7a5fab360179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in np.array_split(scaled_features, len(features) // 500):\n",
    "    ipca.partial_fit(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6758a-531c-42e2-95e9-ff85987af0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'information retained: {sum(ipca.explained_variance_ratio_):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347aadf0-ed4b-44f2-9890-862d92f52f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code performs dimensionality reduction using Incremental Principal Component Analysis (IPCA).\n",
    "# It transforms the scaled feature set into a lower-dimensional space while retaining most of the variance.\n",
    "# A new DataFrame is created from the transformed features, with columns labeled as 'PC1', 'PC2', ..., 'PCn'.\n",
    "# Additionally, an 'Attack Type' column is added to maintain the context of the data for further analysis.\n",
    "\n",
    "transformed_features = ipca.transform(scaled_features)\n",
    "new_data = pd.DataFrame(transformed_features, columns = [f'PC{i+1}' for i in range(size)])\n",
    "new_data['Attack Type'] = attacks.values\n",
    "\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc452cb-46e3-4070-87dc-9f99663b55e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the preprocessed data\n",
    "output_path = r\"E:/Random Python Scripts/CICIDS/CICIDS2017/preprocessed_data.csv\"\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "new_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed data saved successfully at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cicids",
   "language": "python",
   "name": "cicids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
